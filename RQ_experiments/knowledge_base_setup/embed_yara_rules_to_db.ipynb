{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store YARA\n",
    "\n",
    "# Translate YARA Rules to Database Entries with Embeddings\n",
    "\n",
    "This notebook demonstrates how to translate YARA rules into  and generate embeddings for these rules using OpenAI's GPT-4 model. The embeddings will be stored in a PostgreSQL database with the PGVector extension for efficient similarity searches.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to set up the environment by loading necessary libraries and environment variables.\n",
    "\n",
    "Ensure your OpenAI API Key is cofnigured in the .env file as OPENAI_API_KEY\n",
    "\n",
    "This code performs the following steps if the YARA file is yara-rules-core.yar:\n",
    "\n",
    "- Load Environment Variables: It loads environment variables from a .env file, including the OpenAI API key.\n",
    "\n",
    "- Extract YARA Rules: It reads the YARA file and extracts the rules using regular expressions. The extracted rules include the rule name, tags, metadata, detection logic, and condition.\n",
    "\n",
    "- Parse Metadata: It parses the metadata section of each YARA rule to extract key-value pairs.\n",
    "\n",
    "- Parse Strings: It parses the strings section of each YARA rule, decoding any Base64-encoded strings and adding them to the parsed strings.\n",
    "\n",
    "- Interpret Rules Using OpenAI: It sends each YARA rule to the OpenAI API to get an interpretation of the rule, breaking it down into metadata, detection logic, condition, and explanations of encoded strings.\n",
    "\n",
    "- Store Interpreted Rules in PGVector: It connects to a PostgreSQL database with PGVector extension, stores the interpreted rules as documents in the database, and closes the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI,AsyncOpenAI\n",
    "import base64\n",
    "import psycopg2\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import asyncpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "# Ensure your OpenAI API Key is cofnigured in the .env file as OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the connection string to match your postgresql database\n",
    "# PostgreSQL PGVector Config\n",
    "DB_PARAMS = {\n",
    "    \"database\": \"malware_kb\",\n",
    "    \"user\": \"malware_admin\",\n",
    "    \"password\": \"admin_secure_password\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "PGVECTOR_CONNECTION_STRING = f\"postgresql+psycopg://{DB_PARAMS['user']}:{DB_PARAMS['password']}@{DB_PARAMS['host']}:{DB_PARAMS['port']}/{DB_PARAMS['database']}?options=-csearch_path=malware\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse metadata\n",
    "def parse_metadata(metadata_section):\n",
    "    metadata = {}\n",
    "    for line in metadata_section.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if \"=\" in line:\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            metadata[key.strip()] = value.strip().strip('\"')\n",
    "    return metadata\n",
    "\n",
    "# Function to parse and interpret encoded strings\n",
    "def parse_strings(strings_section):\n",
    "    parsed_strings = []\n",
    "    for line in strings_section.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"$\"):\n",
    "            parts = line.split(\"=\", 1)\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                value = value.strip().strip('\"')\n",
    "                if is_base64(value):\n",
    "                    try:\n",
    "                        decoded_value = base64.b64decode(value).decode(\"utf-8\", errors=\"ignore\")\n",
    "                        parsed_strings.append(f\"{key} = \\\"{value}\\\" (decoded: \\\"{decoded_value}\\\")\")\n",
    "                    except Exception:\n",
    "                        parsed_strings.append(f\"{key} = \\\"{value}\\\" (decoded: [failed])\")\n",
    "                else:\n",
    "                    parsed_strings.append(line)\n",
    "    return parsed_strings\n",
    "\n",
    "# Function to check if a string is Base64\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)).decode() == s\n",
    "    except Exception:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interpret_rule(rule, semaphore):\n",
    "    async with semaphore:  # ✅ Proper use of async context manager\n",
    "        prompt = f\"\"\"\n",
    "        Interpret the following YARA rule by breaking it down into:\n",
    "        - **Detection Logic**: Explain in a sentence.\n",
    "        - **Condition**: Explain what must be met for detection.\n",
    "        - **Explanation of Encoded Strings (if any)**: Decode and explain.\n",
    "        - **Overall Explanation**: How does the YARA Rule detect malware?\n",
    "\n",
    "        YARA Rule:\n",
    "        Rule Name: {rule['name']}\n",
    "        Metadata: {rule['metadata']}\n",
    "        Detection Logic: {rule['detection_logic']}\n",
    "        Condition: {rule['condition']}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = await openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity analyst.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=512\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse metadata\n",
    "def parse_metadata(metadata_section):\n",
    "    metadata = {}\n",
    "    for line in metadata_section.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if \"=\" in line:\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            metadata[key.strip()] = value.strip().strip('\"')\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract YARA rules\n",
    "def extract_yara_rules(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Regular expression to match YARA rules\n",
    "    yara_pattern = re.compile(\n",
    "        r\"rule\\s+([\\w_]+)\\s*:\\s*([\\w\\s,]*)\\s*{\\s*meta:\\s*(.*?)\\s*strings:\\s*(.*?)\\s*condition:\\s*(.*?)}\",\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    rules = []\n",
    "    for match in yara_pattern.finditer(content):\n",
    "        rule_name, tags, metadata, detection_logic, condition = match.groups()\n",
    "        rules.append({\n",
    "            \"name\": rule_name.strip(),\n",
    "            \"tags\": tags.strip(),\n",
    "            \"metadata\": parse_metadata(metadata),\n",
    "            \"detection_logic\": parse_strings(detection_logic),\n",
    "            \"condition\": condition.strip()\n",
    "        })\n",
    "    \n",
    "    return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interpret_all_rules(rules, max_concurrent_requests=70,start_range=0,end_range=100):\n",
    "    \"\"\"Interpret YARA rules asynchronously and return a list of results.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_requests)  # ✅ Create semaphore\n",
    "\n",
    "    async def process_rule(rule):\n",
    "        async with semaphore:\n",
    "            interpreted_text = await interpret_rule(rule, semaphore)  # ✅ Interpret rule\n",
    "            return {\n",
    "                \"rule_name\": rule[\"name\"],\n",
    "                \"tags\": rule.get(\"tags\", []),\n",
    "                \"metadata\": rule[\"metadata\"],\n",
    "                \"interpreted_text\": interpreted_text\n",
    "            }\n",
    "\n",
    "    # Run all tasks concurrently and collect results\n",
    "    tasks = [process_rule(rule) for rule in rules[start_range:end_range]]\n",
    "    interpreted_results = await asyncio.gather(*tasks)\n",
    "\n",
    "    return interpreted_results  # ✅ Return all interpreted rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interpretation(interpreted_result):\n",
    "   texts = interpreted_result.split(\"\\n###\")\n",
    "   new_texts = ''\n",
    "   for text in texts:\n",
    "       text = text.strip()\n",
    "       text = text.replace(\".\\n\", \" \")\n",
    "       text = text.replace(\"\\n\", \":\")\n",
    "       new_texts += text + \"\\n\"\n",
    "       \n",
    "   return new_texts\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_in_pgvector(docs):\n",
    "    vector_store = PGVector(\n",
    "        connection=PGVECTOR_CONNECTION_STRING,  # ✅ Use correct connection string\n",
    "        embeddings=embeddings,\n",
    "        collection_name=\"malware.yara_rules2\",\n",
    "        use_jsonb=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    vector_store.add_documents(docs)  # ✅ Store asynchronously\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yara_file_path = \"..\\YaraForge\\sample-yara-rules-core.yar\"  # change this to the path of your YARA file\n",
    "extracted_rules = extract_yara_rules(yara_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting interpretation 20 rules from 6200 to 6221\n",
      "Stored 20 documents in PGVector from 6200 to 6221\n"
     ]
    }
   ],
   "source": [
    "start=6200\n",
    "batch = 21\n",
    "\n",
    "interpreted_results = await interpret_all_rules(extracted_rules, start_range=start, end_range=start+batch)\n",
    "interpreted_documents =[]\n",
    "print(f\"starting interpretation {len(interpreted_results)} rules from {start} to {start+batch}\")\n",
    "for result in interpreted_results:\n",
    "    \n",
    "    metadata={\n",
    "        \"rule_name\": result[\"rule_name\"],\n",
    "        \"tags\": result[\"tags\"],\n",
    "        \"description\": result[\"metadata\"].get(\"description\"),\n",
    "        'author': result[\"metadata\"].get(\"author\"),\n",
    "        'id': result[\"metadata\"].get(\"id\"),\n",
    "        'os': result[\"metadata\"].get(\"os\"),\n",
    "    }\n",
    "    content = process_interpretation(result[\"interpreted_text\"])    \n",
    "    interpreted_documents.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "store_in_pgvector(interpreted_documents)\n",
    "print(f\"Stored {len(interpreted_documents)} documents in PGVector from {start} to {start+batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6220"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate connection string\n",
    "# Use malware schema in PostgreSQL connection string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".malvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
